{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data sets\n",
    "    will be using MNIST data set imported from pytorch. \n",
    "    In the preprocessing stages\n",
    "### TO-DO\n",
    "    how to properly work with images (normally used to numbers)\n",
    "    how to preprocess this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data sets from pytorch. and storing them within the correct variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the data. How many Data Samples do we have? 60000\n",
    "\n",
    "Size of each data? 28x28. Grayscale was already applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating loaders. Used for loading in data. Batch size of about 100 images at a time. This is where each image will be stored for the model we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loaders = {\n",
    "\n",
    "    'train': DataLoader(train_data,\n",
    "                        batch_size = 100,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1),\n",
    "    \n",
    "    'test': DataLoader(train_data,\n",
    "                        batch_size = 100,\n",
    "                        shuffle=True,\n",
    "                        num_workers=1)\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7f12b3e8ce60>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7f12b4098e00>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating class structure for model we will be using. Convolutional Neural network otherwise known as a MLP (Multilayer perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) \n",
    "            # hidden layer 1\n",
    "        self.conv2 = nn.Conv2d(10, 2, kernel_size = 5)  \n",
    "            # hidden layer 2\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        # dropout layer will randomly drop out or deactivate certain neurons during training\n",
    "        # does not change shape of data\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "    def forward(self, x): #manually creating the neurons for CNN\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),2)) # max pooling function\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2)) \n",
    "        # want to reshape and flatten linear data \n",
    "        x = x.view(-1, 320) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization and Training Code block. (More manual since we are using pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    # checking to see if we can use GPU \n",
    "\n",
    "model = CNN().to(device) \n",
    "    # move model to correct architecture\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "    # optimizer with learning rate (0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        print(f'Target size: {target.shape}')\n",
    "        print(f'Output size: {output.shape}')\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100 * batch_idx/ len(loaders[\"train\"]):.0f}%)]\\t{loss.item():.6}')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss  = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct+=pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy {correct}/{len(loaders[\"test\"].dataset)} ({100. * correct/ len(loaders[\"test\"].dataset):.0f}%\\n)')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target size: tensor([3, 1, 6, 2, 6, 4, 4, 2, 8, 2, 7, 0, 8, 4, 9, 8, 2, 1, 8, 1, 0, 5, 0, 4,\n",
      "        9, 3, 1, 0, 1, 2, 6, 5, 3, 6, 5, 6, 8, 4, 8, 9, 1, 6, 8, 2, 9, 8, 8, 8,\n",
      "        0, 2, 6, 0, 6, 0, 5, 7, 0, 9, 1, 1, 6, 1, 3, 7, 0, 0, 7, 1, 3, 0, 6, 7,\n",
      "        4, 2, 5, 9, 1, 4, 4, 5, 6, 0, 7, 1, 0, 8, 5, 5, 5, 2, 0, 9, 6, 8, 0, 2,\n",
      "        8, 0, 3, 2])\n",
      "Output size: tensor([[0.0088, 0.0110, 0.0099, 0.0112, 0.0108, 0.0119, 0.0095, 0.0086, 0.0099,\n",
      "         0.0084, 0.0089, 0.0110, 0.0091, 0.0108, 0.0099, 0.0103, 0.0097, 0.0104,\n",
      "         0.0108, 0.0084, 0.0107, 0.0108, 0.0106, 0.0099, 0.0095, 0.0110, 0.0095,\n",
      "         0.0089, 0.0099, 0.0105, 0.0113, 0.0104, 0.0099, 0.0109, 0.0107, 0.0098,\n",
      "         0.0088, 0.0096, 0.0111, 0.0102, 0.0098, 0.0084, 0.0106, 0.0105, 0.0097,\n",
      "         0.0090, 0.0111, 0.0093, 0.0098, 0.0105, 0.0115, 0.0088, 0.0110, 0.0113,\n",
      "         0.0088, 0.0114, 0.0108, 0.0101, 0.0095, 0.0107, 0.0098, 0.0094, 0.0109,\n",
      "         0.0104, 0.0087, 0.0093, 0.0095, 0.0090, 0.0101, 0.0109, 0.0106, 0.0106,\n",
      "         0.0104, 0.0091, 0.0088, 0.0095, 0.0095, 0.0098, 0.0087, 0.0087, 0.0101,\n",
      "         0.0090, 0.0095, 0.0098, 0.0108, 0.0090, 0.0119, 0.0105, 0.0103, 0.0096,\n",
      "         0.0107, 0.0098, 0.0095, 0.0104, 0.0102, 0.0092, 0.0108, 0.0082, 0.0101,\n",
      "         0.0108],\n",
      "        [0.0080, 0.0115, 0.0102, 0.0110, 0.0100, 0.0123, 0.0100, 0.0095, 0.0100,\n",
      "         0.0084, 0.0082, 0.0117, 0.0079, 0.0121, 0.0093, 0.0101, 0.0093, 0.0113,\n",
      "         0.0106, 0.0094, 0.0105, 0.0102, 0.0113, 0.0101, 0.0086, 0.0107, 0.0098,\n",
      "         0.0085, 0.0093, 0.0097, 0.0116, 0.0097, 0.0094, 0.0110, 0.0118, 0.0095,\n",
      "         0.0088, 0.0095, 0.0104, 0.0111, 0.0097, 0.0075, 0.0106, 0.0106, 0.0099,\n",
      "         0.0090, 0.0111, 0.0093, 0.0096, 0.0112, 0.0118, 0.0082, 0.0111, 0.0110,\n",
      "         0.0092, 0.0117, 0.0100, 0.0105, 0.0088, 0.0111, 0.0094, 0.0095, 0.0110,\n",
      "         0.0100, 0.0092, 0.0093, 0.0095, 0.0091, 0.0097, 0.0111, 0.0109, 0.0102,\n",
      "         0.0108, 0.0093, 0.0095, 0.0104, 0.0086, 0.0103, 0.0091, 0.0090, 0.0107,\n",
      "         0.0097, 0.0093, 0.0097, 0.0109, 0.0092, 0.0115, 0.0110, 0.0106, 0.0099,\n",
      "         0.0099, 0.0090, 0.0090, 0.0098, 0.0098, 0.0093, 0.0100, 0.0088, 0.0114,\n",
      "         0.0099],\n",
      "        [0.0087, 0.0112, 0.0101, 0.0111, 0.0104, 0.0116, 0.0095, 0.0089, 0.0101,\n",
      "         0.0086, 0.0091, 0.0114, 0.0088, 0.0114, 0.0099, 0.0106, 0.0094, 0.0109,\n",
      "         0.0107, 0.0087, 0.0111, 0.0110, 0.0113, 0.0098, 0.0095, 0.0106, 0.0095,\n",
      "         0.0095, 0.0099, 0.0111, 0.0108, 0.0098, 0.0101, 0.0102, 0.0107, 0.0097,\n",
      "         0.0086, 0.0100, 0.0101, 0.0104, 0.0108, 0.0084, 0.0102, 0.0097, 0.0097,\n",
      "         0.0090, 0.0106, 0.0089, 0.0099, 0.0113, 0.0113, 0.0094, 0.0106, 0.0103,\n",
      "         0.0095, 0.0112, 0.0096, 0.0102, 0.0093, 0.0114, 0.0100, 0.0089, 0.0109,\n",
      "         0.0104, 0.0085, 0.0087, 0.0101, 0.0084, 0.0102, 0.0111, 0.0106, 0.0108,\n",
      "         0.0111, 0.0090, 0.0099, 0.0097, 0.0096, 0.0097, 0.0086, 0.0088, 0.0101,\n",
      "         0.0096, 0.0099, 0.0099, 0.0107, 0.0088, 0.0106, 0.0105, 0.0103, 0.0102,\n",
      "         0.0103, 0.0094, 0.0088, 0.0105, 0.0097, 0.0086, 0.0105, 0.0089, 0.0108,\n",
      "         0.0108],\n",
      "        [0.0086, 0.0109, 0.0107, 0.0108, 0.0104, 0.0111, 0.0100, 0.0087, 0.0106,\n",
      "         0.0091, 0.0084, 0.0109, 0.0090, 0.0101, 0.0091, 0.0106, 0.0097, 0.0114,\n",
      "         0.0108, 0.0092, 0.0107, 0.0108, 0.0107, 0.0099, 0.0094, 0.0108, 0.0097,\n",
      "         0.0085, 0.0100, 0.0111, 0.0112, 0.0108, 0.0100, 0.0110, 0.0103, 0.0094,\n",
      "         0.0087, 0.0099, 0.0106, 0.0106, 0.0103, 0.0089, 0.0104, 0.0100, 0.0101,\n",
      "         0.0090, 0.0104, 0.0090, 0.0098, 0.0104, 0.0111, 0.0092, 0.0113, 0.0107,\n",
      "         0.0091, 0.0113, 0.0104, 0.0097, 0.0088, 0.0110, 0.0098, 0.0092, 0.0107,\n",
      "         0.0109, 0.0084, 0.0091, 0.0097, 0.0090, 0.0109, 0.0112, 0.0110, 0.0103,\n",
      "         0.0107, 0.0088, 0.0088, 0.0095, 0.0097, 0.0102, 0.0087, 0.0082, 0.0098,\n",
      "         0.0096, 0.0096, 0.0096, 0.0105, 0.0090, 0.0111, 0.0110, 0.0104, 0.0091,\n",
      "         0.0109, 0.0096, 0.0098, 0.0103, 0.0097, 0.0090, 0.0110, 0.0085, 0.0104,\n",
      "         0.0112],\n",
      "        [0.0083, 0.0112, 0.0102, 0.0104, 0.0098, 0.0109, 0.0102, 0.0088, 0.0102,\n",
      "         0.0085, 0.0089, 0.0110, 0.0092, 0.0107, 0.0088, 0.0108, 0.0091, 0.0108,\n",
      "         0.0114, 0.0086, 0.0099, 0.0104, 0.0114, 0.0095, 0.0095, 0.0110, 0.0099,\n",
      "         0.0092, 0.0092, 0.0109, 0.0111, 0.0101, 0.0093, 0.0114, 0.0107, 0.0094,\n",
      "         0.0086, 0.0099, 0.0115, 0.0102, 0.0101, 0.0085, 0.0101, 0.0100, 0.0097,\n",
      "         0.0097, 0.0115, 0.0088, 0.0099, 0.0104, 0.0115, 0.0091, 0.0116, 0.0103,\n",
      "         0.0091, 0.0108, 0.0095, 0.0099, 0.0096, 0.0107, 0.0100, 0.0093, 0.0101,\n",
      "         0.0109, 0.0088, 0.0086, 0.0098, 0.0085, 0.0111, 0.0118, 0.0106, 0.0098,\n",
      "         0.0110, 0.0087, 0.0097, 0.0099, 0.0101, 0.0107, 0.0088, 0.0089, 0.0107,\n",
      "         0.0099, 0.0097, 0.0095, 0.0112, 0.0094, 0.0111, 0.0109, 0.0106, 0.0097,\n",
      "         0.0102, 0.0095, 0.0099, 0.0098, 0.0092, 0.0092, 0.0104, 0.0083, 0.0104,\n",
      "         0.0112],\n",
      "        [0.0087, 0.0111, 0.0100, 0.0112, 0.0106, 0.0118, 0.0093, 0.0088, 0.0103,\n",
      "         0.0086, 0.0091, 0.0108, 0.0087, 0.0110, 0.0101, 0.0105, 0.0096, 0.0111,\n",
      "         0.0108, 0.0091, 0.0111, 0.0107, 0.0108, 0.0096, 0.0093, 0.0107, 0.0099,\n",
      "         0.0092, 0.0100, 0.0106, 0.0114, 0.0103, 0.0097, 0.0108, 0.0110, 0.0097,\n",
      "         0.0089, 0.0102, 0.0104, 0.0104, 0.0100, 0.0084, 0.0109, 0.0101, 0.0098,\n",
      "         0.0089, 0.0106, 0.0092, 0.0095, 0.0110, 0.0115, 0.0089, 0.0108, 0.0112,\n",
      "         0.0093, 0.0110, 0.0105, 0.0099, 0.0088, 0.0110, 0.0095, 0.0096, 0.0112,\n",
      "         0.0104, 0.0088, 0.0088, 0.0096, 0.0088, 0.0099, 0.0104, 0.0107, 0.0105,\n",
      "         0.0106, 0.0090, 0.0090, 0.0093, 0.0092, 0.0098, 0.0089, 0.0085, 0.0100,\n",
      "         0.0094, 0.0094, 0.0101, 0.0110, 0.0087, 0.0112, 0.0109, 0.0105, 0.0095,\n",
      "         0.0103, 0.0095, 0.0095, 0.0101, 0.0098, 0.0092, 0.0108, 0.0088, 0.0106,\n",
      "         0.0109],\n",
      "        [0.0086, 0.0109, 0.0096, 0.0111, 0.0107, 0.0120, 0.0093, 0.0086, 0.0106,\n",
      "         0.0082, 0.0094, 0.0109, 0.0089, 0.0114, 0.0097, 0.0106, 0.0095, 0.0111,\n",
      "         0.0111, 0.0091, 0.0108, 0.0101, 0.0110, 0.0095, 0.0095, 0.0106, 0.0097,\n",
      "         0.0089, 0.0098, 0.0112, 0.0114, 0.0104, 0.0097, 0.0106, 0.0111, 0.0096,\n",
      "         0.0082, 0.0097, 0.0103, 0.0106, 0.0104, 0.0083, 0.0104, 0.0101, 0.0097,\n",
      "         0.0086, 0.0112, 0.0086, 0.0097, 0.0112, 0.0114, 0.0092, 0.0111, 0.0106,\n",
      "         0.0095, 0.0114, 0.0098, 0.0105, 0.0093, 0.0109, 0.0097, 0.0092, 0.0105,\n",
      "         0.0103, 0.0089, 0.0089, 0.0097, 0.0088, 0.0102, 0.0111, 0.0103, 0.0099,\n",
      "         0.0110, 0.0094, 0.0094, 0.0094, 0.0097, 0.0100, 0.0088, 0.0089, 0.0106,\n",
      "         0.0098, 0.0098, 0.0093, 0.0119, 0.0084, 0.0107, 0.0106, 0.0103, 0.0099,\n",
      "         0.0104, 0.0096, 0.0094, 0.0104, 0.0098, 0.0090, 0.0106, 0.0085, 0.0110,\n",
      "         0.0105],\n",
      "        [0.0087, 0.0116, 0.0100, 0.0110, 0.0107, 0.0113, 0.0094, 0.0090, 0.0099,\n",
      "         0.0086, 0.0084, 0.0112, 0.0089, 0.0111, 0.0095, 0.0107, 0.0093, 0.0113,\n",
      "         0.0107, 0.0086, 0.0106, 0.0111, 0.0110, 0.0097, 0.0095, 0.0108, 0.0100,\n",
      "         0.0088, 0.0097, 0.0106, 0.0110, 0.0102, 0.0100, 0.0106, 0.0107, 0.0095,\n",
      "         0.0086, 0.0105, 0.0104, 0.0105, 0.0102, 0.0088, 0.0103, 0.0103, 0.0093,\n",
      "         0.0090, 0.0105, 0.0095, 0.0100, 0.0109, 0.0112, 0.0094, 0.0114, 0.0111,\n",
      "         0.0093, 0.0109, 0.0103, 0.0099, 0.0088, 0.0112, 0.0098, 0.0094, 0.0111,\n",
      "         0.0105, 0.0089, 0.0088, 0.0101, 0.0090, 0.0102, 0.0112, 0.0105, 0.0103,\n",
      "         0.0107, 0.0092, 0.0090, 0.0096, 0.0092, 0.0099, 0.0084, 0.0087, 0.0099,\n",
      "         0.0093, 0.0094, 0.0103, 0.0108, 0.0090, 0.0105, 0.0109, 0.0108, 0.0094,\n",
      "         0.0106, 0.0097, 0.0093, 0.0104, 0.0097, 0.0091, 0.0109, 0.0084, 0.0104,\n",
      "         0.0107],\n",
      "        [0.0085, 0.0122, 0.0098, 0.0111, 0.0108, 0.0117, 0.0096, 0.0085, 0.0100,\n",
      "         0.0083, 0.0084, 0.0112, 0.0093, 0.0107, 0.0096, 0.0107, 0.0094, 0.0107,\n",
      "         0.0105, 0.0082, 0.0106, 0.0112, 0.0112, 0.0094, 0.0093, 0.0117, 0.0099,\n",
      "         0.0090, 0.0098, 0.0106, 0.0110, 0.0102, 0.0093, 0.0115, 0.0102, 0.0095,\n",
      "         0.0093, 0.0103, 0.0108, 0.0105, 0.0102, 0.0083, 0.0108, 0.0098, 0.0096,\n",
      "         0.0094, 0.0108, 0.0098, 0.0096, 0.0106, 0.0111, 0.0095, 0.0107, 0.0106,\n",
      "         0.0094, 0.0105, 0.0107, 0.0098, 0.0088, 0.0106, 0.0098, 0.0092, 0.0109,\n",
      "         0.0103, 0.0086, 0.0085, 0.0097, 0.0084, 0.0107, 0.0112, 0.0106, 0.0100,\n",
      "         0.0105, 0.0089, 0.0096, 0.0097, 0.0097, 0.0105, 0.0084, 0.0088, 0.0102,\n",
      "         0.0096, 0.0098, 0.0095, 0.0104, 0.0093, 0.0114, 0.0114, 0.0106, 0.0097,\n",
      "         0.0100, 0.0097, 0.0101, 0.0097, 0.0095, 0.0093, 0.0111, 0.0080, 0.0100,\n",
      "         0.0111],\n",
      "        [0.0083, 0.0117, 0.0104, 0.0111, 0.0106, 0.0118, 0.0099, 0.0094, 0.0103,\n",
      "         0.0089, 0.0081, 0.0113, 0.0085, 0.0113, 0.0094, 0.0105, 0.0096, 0.0117,\n",
      "         0.0103, 0.0090, 0.0107, 0.0104, 0.0115, 0.0097, 0.0090, 0.0108, 0.0100,\n",
      "         0.0087, 0.0098, 0.0105, 0.0114, 0.0099, 0.0098, 0.0110, 0.0110, 0.0095,\n",
      "         0.0088, 0.0096, 0.0105, 0.0109, 0.0105, 0.0082, 0.0107, 0.0103, 0.0097,\n",
      "         0.0090, 0.0107, 0.0091, 0.0097, 0.0112, 0.0118, 0.0087, 0.0112, 0.0110,\n",
      "         0.0095, 0.0110, 0.0100, 0.0103, 0.0087, 0.0113, 0.0095, 0.0096, 0.0107,\n",
      "         0.0106, 0.0086, 0.0091, 0.0096, 0.0087, 0.0103, 0.0111, 0.0108, 0.0100,\n",
      "         0.0109, 0.0089, 0.0096, 0.0102, 0.0090, 0.0102, 0.0087, 0.0088, 0.0103,\n",
      "         0.0097, 0.0094, 0.0094, 0.0108, 0.0091, 0.0109, 0.0107, 0.0103, 0.0096,\n",
      "         0.0098, 0.0093, 0.0094, 0.0097, 0.0096, 0.0090, 0.0106, 0.0087, 0.0108,\n",
      "         0.0103]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87505/1583925582.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (10) to match target batch_size (100).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     test()\n",
      "Cell \u001b[0;32mIn[44], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (10) to match target batch_size (100)."
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 320]' is invalid for input of size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m data, target \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m prediction \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediciton: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x),\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)),\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# want to retunr flat linear data \u001b[39;00m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 320]' is invalid for input of size 32"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "data, target = test_data[0]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim=1, keepdim=True).item\n",
    "\n",
    "print(f'Prediciton: {prediction}')\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap = 'gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
